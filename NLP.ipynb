{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIAerospace/LLM/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0u1kuRaT-3U"
      },
      "source": [
        "# NLP a la vieja usanza\n",
        "\n",
        "Vamos a analizar textos sin utilizar LLMs.\n",
        "\n",
        "Creamos un texto de ejemplo y lo cargamos en spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenization\n",
        "from spacy.lang.es import Spanish\n",
        "\n",
        "# Cargar tokenizer, tagger, parser, NER y embedings\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "nlp = Spanish()\n",
        "\n",
        "text = \"\"\"María tenía un corderito blanco como la nieve. Los tipos que fuman puro tienen cara de canguro.\n",
        "Nunca vi a un corderito fumar en puro. La nieve es blanca y suave.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n"
      ],
      "metadata": {
        "id": "YYRtk_L7ULxR",
        "outputId": "1a225126-a543-40e8-cba8-6fc74a2761b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-35ed4a1b158c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Cargar tokenizer, tagger, parser, NER y embedings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpanish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de palabra\n",
        "token_list = []\n",
        "for token in doc:\n",
        "    token_list.append(token.text)\n",
        "print(token_list)"
      ],
      "metadata": {
        "id": "pgSwug0jU2Gk",
        "outputId": "9f80ae46-80f4-487f-84ed-86f8eb358e55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['María', 'tenía', 'un', 'corderito', 'blanco', 'como', 'la', 'nieve', '.', 'Los', 'tipos', 'que', 'fuman', 'puro', 'tienen', 'cara', 'de', 'canguro', '.', '\\n', 'Nunca', 'vi', 'a', 'un', 'corderito', 'fumar', 'en', 'puro', '.', 'La', 'nieve', 'es', 'blanca', 'y', 'suave', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop words\n",
        "\n",
        "import spacy\n",
        "spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "\n",
        "\n",
        "print('Numero de stop words: %d' % len(spacy_stopwords))\n",
        "print('priemros 10 stop words: %s' % list(spacy_stopwords)[:10])\n",
        "\n",
        "\n",
        "#Lo aplicamos a nuestro texto\n",
        "texto_filtrado=[]\n",
        "\n",
        "# filtrando\n",
        "for word in doc:\n",
        "    if word.is_stop==False:\n",
        "        texto_filtrado.append(word)\n",
        "print(\"Resultado:\",texto_filtrado)"
      ],
      "metadata": {
        "id": "jnP3uOzPV7hu",
        "outputId": "41ab6ecc-fff7-4c49-dbeb-2b502deadc2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de stop words: 521\n",
            "priemros 10 stop words: ['mucho', 'cuanta', 'debe', 'su', 'tres', 'otro', 'día', 'ese', 'sí', 'temprano']\n",
            "Resultado: [María, corderito, blanco, nieve, ., tipos, fuman, puro, cara, canguro, ., \n",
            ", vi, corderito, fumar, puro, ., nieve, blanca, suave, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing lemmatization\n",
        "lem = nlp(\"run runs running runner\")\n",
        "# finding lemma for each word\n",
        "for word in lem:\n",
        "    print(word.text,word.lemma_)"
      ],
      "metadata": {
        "id": "v_rWMksUW6KC",
        "outputId": "4f809377-f59b-4607-d2ed-8819edac2bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run \n",
            "runs \n",
            "running \n",
            "runner \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrqUB-3mUCYr"
      },
      "source": [
        "# Clasificación de textos\n",
        "\n",
        "Cargamos el dataset. Contiene textos escritos por tres autores diferentes de la misma época y estilo literario:\n",
        "\n",
        "* Edgar Alan Poe (EAP)\n",
        "* H.P. Lovecraft (HPL)\n",
        "* Mary Shelley (MWS)\n",
        "\n",
        "El objetivo es entrenar un modelo que se capaz de reconocer entre estos tres el autor de un texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ0U6uLFT-3X",
        "outputId": "f0082bd5-7789-4dcd-eb17-a260c87dcee0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"Data/train.csv\")\n",
        "test = pd.read_csv(\"Data/test.csv\")\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxV5lxwyT-3Z",
        "outputId": "c439166a-75ca-44fd-c639-5993ffb9736a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19579, 3)\n",
            "(8392, 2)\n",
            "author\n",
            "EAP    7900\n",
            "MWS    6044\n",
            "HPL    5635\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(train.shape)\n",
        "print(test.shape)\n",
        "print(train['author'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoBwTe7dT-3Z"
      },
      "source": [
        "### Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGgXI_KaT-3a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "          'María tenía un corderito blanco como la nieve',\n",
        "          'Los tipos que fuman puro tienen cara de canguro',\n",
        "          'Nunca vi a un corderito fumar en puro',\n",
        "          'La nieve es blanca y suave',\n",
        " ]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer2 = CountVectorizer()\n",
        "\n",
        "# TD-IDF Matrix\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "\n",
        "# extracting feature names\n",
        "tfidf_tokens = vectorizer.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WevRuEQnT-3a",
        "outputId": "34e9403d-f178-48e3-b672-5223f6981e4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
              "        1, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
              "        0, 0],\n",
              "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "        1, 1],\n",
              "       [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
              "        0, 0]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X2.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT518ol_T-3a",
        "outputId": "8e5a802a-9571-4686-f876-ebb538da5fdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.        , 0.39264414, 0.        , 0.        , 0.39264414,\n",
              "        0.30956515, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.30956515, 0.        , 0.39264414, 0.30956515,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.39264414,\n",
              "        0.        , 0.        , 0.30956515, 0.        ],\n",
              "       [0.        , 0.        , 0.34056989, 0.34056989, 0.        ,\n",
              "        0.        , 0.34056989, 0.        , 0.        , 0.34056989,\n",
              "        0.        , 0.        , 0.34056989, 0.        , 0.        ,\n",
              "        0.        , 0.26850921, 0.34056989, 0.        , 0.        ,\n",
              "        0.34056989, 0.34056989, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.32555709, 0.        , 0.41292788, 0.        , 0.        ,\n",
              "        0.41292788, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.41292788, 0.32555709, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.32555709, 0.41292788],\n",
              "       [0.48546061, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.48546061, 0.        ,\n",
              "        0.        , 0.38274272, 0.        , 0.        , 0.38274272,\n",
              "        0.        , 0.        , 0.        , 0.48546061, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmEiqTXgT-3b",
        "outputId": "525da7ab-a670-4f43-9f05-f515558e78f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['blanca', 'blanco', 'canguro', 'cara', 'como', 'corderito', 'de',\n",
              "       'en', 'es', 'fuman', 'fumar', 'la', 'los', 'maría', 'nieve',\n",
              "       'nunca', 'puro', 'que', 'suave', 'tenía', 'tienen', 'tipos', 'un',\n",
              "       'vi'], dtype=object)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5mZDWaOT-3b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}