{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Invocar los modelos directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao, come stai?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from Spanish into Italian\"),\n",
    "    HumanMessage(\"Hola, ¿cómo estás?\"),\n",
    "]\n",
    "\n",
    "respuesta = model.invoke(messages)\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Plantillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen:\n",
      "Librería LangChain facilita aplicaciones basadas en modelos de lenguaje AI.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "# Define una plantilla para el prompt que solicite resumir un texto\n",
    "template = (\n",
    "    \"Resume el siguiente texto en 10 palabras:\\n\\n\"\n",
    "    \"{text}\\n\\n\"\n",
    "    \"Resumen:\"\n",
    ")\n",
    "prompt = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "\n",
    "# Crea una instancia del modelo de lenguaje de OpenAI (puedes ajustar temperature y modelo)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "\n",
    "# Crea la cadena LLMChain utilizando el modelo y la plantilla\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Define el texto a resumir\n",
    "texto = (\n",
    "    \"LangChain es una librería que facilita la construcción de aplicaciones basadas en modelos de lenguaje. \"\n",
    "    \"Proporciona herramientas para el manejo de prompts, encadenamiento de operaciones y la integración con la API de OpenAI. \"\n",
    "    \"Es especialmente útil para desarrolladores que desean crear flujos de trabajo complejos y personalizados utilizando inteligencia artificial.\"\n",
    ")\n",
    "\n",
    "# Ejecuta la cadena para generar el resumen del texto\n",
    "resumen = chain.run(text=texto)\n",
    "\n",
    "print(\"Resumen:\")\n",
    "print(resumen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: ¡Hola! ¿En qué puedo ayudarte hoy? (Escribe 'salir' para terminar.)\n",
      "Chatbot: La capital de Ruanda es Kigali. Es una ciudad situada en el centro del país y es conocida por su limpieza y orden. Kigali se ha desarrollado rápidamente en las últimas décadas y es un importante centro político, económico y cultural de Ruanda. Además, la ciudad se encuentra en una región montañosa, lo que le brinda un paisaje impresionante. ¿Te gustaría saber más sobre Ruanda o su cultura?\n",
      "Chatbot: ¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas necesarias\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# Configurar el modelo de lenguaje\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.6)\n",
    "\n",
    "# Inicializar memoria de conversación para recordar el contexto\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Crear una cadena de conversación usando el modelo y la memoria\n",
    "chat = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# Iniciar conversación interactiva\n",
    "print(\"Chatbot: ¡Hola! ¿En qué puedo ayudarte hoy? (Escribe 'salir' para terminar.)\")\n",
    "\n",
    "while True:\n",
    "    user_message = input(\"Tú: \")\n",
    "    \n",
    "    if user_message.lower() == \"salir\":\n",
    "        print(\"Chatbot: ¡Hasta luego!\")\n",
    "        break\n",
    "    \n",
    "    # Obtener respuesta usando LangChain\n",
    "    response = chat.predict(input=user_message)\n",
    "    \n",
    "    print(\"Chatbot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
